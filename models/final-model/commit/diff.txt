diff --git a/pexps/env.py b/pexps/env.py
index 4f3c1be..c4632f8 100644
--- a/pexps/env.py
+++ b/pexps/env.py
@@ -65,7 +65,7 @@ class Env:
         i=0
         for i in range(self.c.warmup_steps):
             ret = self.step(warmup=True)
-        print("Warmed up for " + str(i) + " steps" )
+        print("Warmed up for " + str(i + 1)  + " steps" )
        
         if isinstance(ret, tuple):
             return ret[0]
diff --git a/pexps/exp.py b/pexps/exp.py
index bc24e41..89a9d2a 100644
--- a/pexps/exp.py
+++ b/pexps/exp.py
@@ -91,7 +91,6 @@ class Main(Config):
 
     def on_rollout_worker_start(c):
         c._env = c.create_env()
-        c.use_critic = False # Don't need value function on workers
         c.set_model()
         c._model.eval()
         c._i = 0
@@ -102,8 +101,7 @@ class Main(Config):
     def on_train_start(c):
         """
         Run at the begining of the simulation run. Will create the environment and 
-        initialize the training algorithm. Also can initilaize tensorboard and wandb if the right 
-        flags are set to True.  
+        initialize the training algorithm. 
         """
         c.setdefaults(alg='Algorithm')
         c._env = c.create_env()
@@ -139,7 +137,7 @@ class Main(Config):
         Record training stats and save the training model
         """
         lr = c._lr
-        for g in c._opt.param_groups:
+        for g in c._opt.param_groups: 
             g['lr'] = float(lr)
         c.log_stats(dict(**stats, **c._alg.on_step_start(), lr=lr))
         try:
@@ -163,6 +161,7 @@ class Main(Config):
         c.avg_reward = 0
         c.velocity_fleet = 0
         c.rl_speed = 0
+        c.fuel_fleet = 0
 
         if c._i % c.step_save == 0:
             c.save_train_results(c._results)
@@ -219,17 +218,14 @@ class Main(Config):
         step = 0
         # rollout unfold 
         while step < c.horizon + c.skip_stat_steps and not done:
-            if len(rollout.obs[-1]) != 0:
-                pred = from_torch(c._model(to_torch(rollout.obs[-1]), value=c.use_critic, policy=True, argmax=False))
-                if c.get('aclip', True) and isinstance(a_space, Box):
-                    pred.action = np.clip(pred.action, a_space.low, a_space.high)
-            else:
-                t = np.array([])
-                pol = arrayf(t.reshape(-1, 2))
-                acti = arrayf(t.reshape(-1, 1))
-                pred = {'policy':pol, 'action':acti} 
+
+            pred = from_torch(c._model(to_torch(rollout.obs[-1]), value=False, policy=True, argmax=False))
+
+            if c.get('aclip', True) and isinstance(a_space, Box):
+                # TODO: check if this works 
+                pred.action = np.clip(pred.action, a_space.low, a_space.high)
             rollout.append(**pred)
-            ret = c._env.step(rollout.action[-1], rollout.id[-1], worker_id=worker_id)
+            ret = c._env.step(rollout.action[-1], rollout.id[-1])
 
             if isinstance(ret, tuple):
                 obs, reward, done, info = ret
@@ -238,15 +234,21 @@ class Main(Config):
             if done:
                 ret = {k: v for k, v in ret.items() if k not in ['obs', 'id']}
             rollout.append(**ret)
-        
             step += 1
 
         stats = {"steps": step}
         
         print("Avg overall speed : " + str(c.velocity_fleet/c.running_steps))
         print("Avg rl speed : " + str(c.rl_speed/c.running_steps))
+        print("Sum fuel : " + str(c.fuel_fleet))
         print("Avg reward : " + str(c.avg_reward/c.running_steps))
 
+        c.running_steps = 0 
+        c.avg_reward = 0
+        c.velocity_fleet = 0
+        c.rl_speed = 0
+        c.fuel_fleet = 0
+        
         return rollout, stats
 
     def on_rollout_end(c, rollout, stats, ii=None, n_ii=None):
@@ -269,7 +271,7 @@ class Main(Config):
 
         if multi_agent:
             step_n = [len(x) for x in rollout.reward]
-            reward = np.concatenate(rollout.reward)
+            reward = (np.concatenate(rollout.reward)).flatten()
             ret, adv = calc_adv_multi_agent(np.concatenate(step_id_), reward, c.gamma, value_=value_, lam=c.lam)
             rollout.update(obs=step_obs, ret=split(ret, step_n))
             if c.use_critic:
@@ -367,12 +369,14 @@ class Main(Config):
 
             print("Avg overall speed : " + str(c.velocity_fleet/c.running_steps))
             print("Avg rl speed : " + str(c.rl_speed/c.running_steps))
+            print("Sum fuel : " + str(c.fuel_fleet))
             print("Avg reward : " + str(c.avg_reward/c.running_steps))
 
             c.running_steps = 0 
             c.avg_reward = 0
             c.velocity_fleet = 0
             c.rl_speed = 0
+            c.fuel_fleet = 0
 
         if hasattr(c._env, 'close'):
             c._env.close()
@@ -382,7 +386,7 @@ class Main(Config):
         Selects training or evaluation route based on provided inputs 
         """
         c.log(format_yaml({k: v for k, v in c.items() if not k.startswith('_')}))
-        c.setdefaults(n_rollouts_per_step=c.num_workers)
+        c.setdefaults(n_rollouts_per_step=c.per_step_rollouts)
         if c.test_run: 
             c.setdefaults(n_workers=1)
             c.setdefaults(use_ray=False, n_rollouts_per_worker=c.n_rollouts_per_step // c.n_workers)
diff --git a/pexps/main.py b/pexps/main.py
index 7f2c3b1..f49fb7a 100644
--- a/pexps/main.py
+++ b/pexps/main.py
@@ -5,7 +5,8 @@ from scenarios.no_stop import NoStopNetwork
 
 if __name__ == '__main__':
     c = NoStopNetwork.from_args(globals(), locals())
-    n_workers = 5
+    # set the number of workers here
+    n_workers = 10
     c.setdefaults( 
  
         sim_step=0.5, 
@@ -14,43 +15,58 @@ if __name__ == '__main__':
         render=False,
         step_save=5,
         use_ray=True,
-        num_workers=n_workers,    
+        num_workers=n_workers,
+        per_step_rollouts=n_workers*1, 
 
-        max_accel=3.0,
-        max_decel=3.0,
+        max_accel=5.0,
+        max_decel=-5.0,
 
-        alg=PG,
-        n_gds=1,
-        n_minibatches=8,
+        alg=TRPO,
+        n_gds=10,
+        n_minibatches=40,
         lr=1e-3,
-        gamma=0.999,
+        gamma=0.99,
+        lam=0.97,
         opt='Adam',
-        norm_reward=True,
-        center_reward=True,
+        norm_reward=False,
+        center_reward=False,
 
         _n_obs=11,
-        n_actions=5,
-        n_steps=1000,
-        horizon=300,
+        n_steps=3000,
+        horizon=600, #2000
         warmup_steps=50,
         act_type='accel',
         sumo_dir="sumo",
-        e=60,
+        e=210,
         test_run=False,
         wandb=False,
-        wandb_dry_run=False,
+        wandb_dry_run=True,
         wandb_proj="no-stop-intersections",
-        wandb_run="test",
 
-        seed=2000,
         target_vel=15.0, 
         avg_reward=0,
         running_steps=0,
         velocity_fleet=0,
         rl_speed=0,
+        fuel_fleet=0,
     )
     if c.test_run:
         c.use_ray = False
         print("Override \"use_ray\" variable if in test mode")
-    assert c.alg == PG, 'Not supporting value functions yet'
+
+    if c.alg == "PPO" or c.alg == "TRPO":
+        c.use_critic = True
+
+    # for reproducibility, set the seed      
+    import torch
+    import numpy as np
+
+    #random_seed = 3000 
+    #torch.manual_seed(random_seed)
+    #torch.cuda.manual_seed(random_seed)
+    #torch.backends.cudnn.deterministic = True
+    #torch.backends.cudnn.benchmark = False
+    #np.random.seed(random_seed)
+
+    # run experiment
     c.run()
\ No newline at end of file
diff --git a/pexps/ppo-test.py b/pexps/ppo-test.py
deleted file mode 100644
index 09d9a6e..0000000
--- a/pexps/ppo-test.py
+++ /dev/null
@@ -1,476 +0,0 @@
-import multiprocessing
-import multiprocessing.connection
-from typing import Dict, List
-
-import cv2
-import gym
-import numpy as np
-import torch
-from labml import monit, tracker, logger, experiment
-from torch import nn
-from torch import optim
-from torch.distributions import Categorical
-from torch.nn import functional as F
-
-if torch.cuda.is_available():
-    device = torch.device("cuda:1")
-else:
-    device = torch.device("cpu")
-
-
-class Game:
-    """
-    This is a wrapper for OpenAI gym game environment.
-    We do a few things here:
-    1. Apply the same action on four frames and get the last frame
-    2. Convert observation frames to gray and scale it to (84, 84)
-    3. Stack four frames of the last four actions
-    4. Add episode information (total reward for the entire episode) for monitoring
-    5. Restrict an episode to a single life (game has 5 lives, we reset after every single life)
-    #### Observation format
-    Observation is tensor of size (4, 84, 84). It is four frames
-    (images of the game screen) stacked on first axis.
-    i.e, each channel is a frame.
-    """
-
-    def __init__(self, seed: int):
-        # create environment
-        self.env = gym.make('BreakoutNoFrameskip-v4')
-        self.env.seed(seed)
-
-        # tensor for a stack of 4 frames
-        self.obs_4 = np.zeros((4, 84, 84))
-
-        # keep track of the episode rewards
-        self.rewards = []
-        # and number of lives left
-        self.lives = 0
-
-    def step(self, action):
-        """
-        ### Step
-        Executes `action` for 4 time steps and
-         returns a tuple of (observation, reward, done, episode_info).
-        * `observation`: stacked 4 frames (this frame and frames for last 3 actions)
-        * `reward`: total reward while the action was executed
-        * `done`: whether the episode finished (a life lost)
-        * `episode_info`: episode information if completed
-        """
-
-        reward = 0.
-        done = None
-
-        # run for 4 steps
-        for i in range(4):
-            # execute the action in the OpenAI Gym environment
-            obs, r, done, info = self.env.step(action)
-
-            reward += r
-
-            # get number of lives left
-            lives = self.env.unwrapped.ale.lives()
-            # reset if a life is lost
-            if lives < self.lives:
-                done = True
-                break
-
-        # Transform the last observation to (84, 84)
-        obs = self._process_obs(obs)
-
-        # maintain rewards for each step
-        self.rewards.append(reward)
-
-        if done:
-            # if finished, set episode information if episode is over, and reset
-            episode_info = {"reward": sum(self.rewards), "length": len(self.rewards)}
-            self.reset()
-        else:
-            episode_info = None
-            # get the max of last two frames
-            # obs = self.obs_2_max.max(axis=0)
-
-            # push it to the stack of 4 frames
-            self.obs_4 = np.roll(self.obs_4, shift=-1, axis=0)
-            self.obs_4[-1] = obs
-
-        return self.obs_4, reward, done, episode_info
-
-    def reset(self):
-        """
-        ### Reset environment
-        Clean up episode info and 4 frame stack
-        """
-
-        # reset OpenAI Gym environment
-        obs = self.env.reset()
-
-        # reset caches
-        obs = self._process_obs(obs)
-        for i in range(4):
-            self.obs_4[i] = obs
-        self.rewards = []
-
-        self.lives = self.env.unwrapped.ale.lives()
-
-        return self.obs_4
-
-    @staticmethod
-    def _process_obs(obs):
-        """
-        #### Process game frames
-        Convert game frames to gray and rescale to 84x84
-        """
-        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
-        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)
-        return obs
-
-
-def worker_process(remote: multiprocessing.connection.Connection, seed: int):
-    """
-    ##Worker Process
-    Each worker process runs this method
-    """
-
-    # create game
-    game = Game(seed)
-
-    # wait for instructions from the connection and execute them
-    while True:
-        cmd, data = remote.recv()
-        if cmd == "step":
-            remote.send(game.step(data))
-        elif cmd == "reset":
-            remote.send(game.reset())
-        elif cmd == "close":
-            remote.close()
-            break
-        else:
-            raise NotImplementedError
-
-
-class Worker:
-    """
-    Creates a new worker and runs it in a separate process.
-    """
-
-    def __init__(self, seed):
-        self.child, parent = multiprocessing.Pipe()
-        self.process = multiprocessing.Process(target=worker_process, args=(parent, seed))
-        self.process.start()
-
-
-class Model(nn.Module):
-    """
-    ## Model
-    """
-
-    def __init__(self):
-        super().__init__()
-
-        # The first convolution layer takes a
-        # 84x84 frame and produces a 20x20 frame
-        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)
-
-        # The second convolution layer takes a
-        # 20x20 frame and produces a 9x9 frame
-        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)
-
-        # The third convolution layer takes a
-        # 9x9 frame and produces a 7x7 frame
-        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)
-
-        # A fully connected layer takes the flattened
-        # frame from third convolution layer, and outputs
-        # 512 features
-        self.lin = nn.Linear(in_features=7 * 7 * 64, out_features=512)
-
-        # A fully connected layer to get logits for $\pi$
-        self.pi_logits = nn.Linear(in_features=512, out_features=4)
-
-        # A fully connected layer to get value function
-        self.value = nn.Linear(in_features=512, out_features=1)
-
-    def forward(self, obs: torch.Tensor):
-        h = F.relu(self.conv1(obs))
-        h = F.relu(self.conv2(h))
-        h = F.relu(self.conv3(h))
-        h = h.reshape((-1, 7 * 7 * 64))
-
-        h = F.relu(self.lin(h))
-
-        pi = Categorical(logits=self.pi_logits(h))
-        value = self.value(h).reshape(-1)
-
-        return pi, value
-
-
-def obs_to_torch(obs: np.ndarray) -> torch.Tensor:
-    # scale to `[0, 1]`
-    return torch.tensor(obs, dtype=torch.float32, device=device) / 255.
-
-
-class Main:
-    def __init__(self):
-        # #### Configurations
-
-        # $\gamma$ and $\lambda$ for advantage calculation
-        self.gamma = 0.99
-        self.lamda = 0.95
-
-        # number of updates
-        self.updates = 10000
-        # number of epochs to train the model with sampled data
-        self.epochs = 4
-        # number of worker processes
-        self.n_workers = 8
-        # number of steps to run on each process for a single update
-        self.worker_steps = 128
-        # number of mini batches
-        self.n_mini_batch = 4
-        # total number of samples for a single update
-        self.batch_size = self.n_workers * self.worker_steps
-        # size of a mini batch
-        self.mini_batch_size = self.batch_size // self.n_mini_batch
-        assert (self.batch_size % self.n_mini_batch == 0)
-
-        # #### Initialize
-
-        # create workers
-        self.workers = [Worker(47 + i) for i in range(self.n_workers)]
-
-        # initialize tensors for observations
-        self.obs = np.zeros((self.n_workers, 4, 84, 84), dtype=np.uint8)
-        for worker in self.workers:
-            worker.child.send(("reset", None))
-        for i, worker in enumerate(self.workers):
-            self.obs[i] = worker.child.recv()
-
-        # model for sampling
-        self.model = Model().to(device)
-
-        # optimizer
-        self.optimizer = optim.Adam(self.model.parameters(), lr=2.5e-4)
-
-    def sample(self):
-        """### Sample data with current policy"""
-
-        rewards = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
-        actions = np.zeros((self.n_workers, self.worker_steps), dtype=np.int32)
-        done = np.zeros((self.n_workers, self.worker_steps), dtype=np.bool)
-        obs = np.zeros((self.n_workers, self.worker_steps, 4, 84, 84), dtype=np.uint8)
-        log_pis = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
-        values = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
-
-        # sample `worker_steps` from each worker
-        for t in range(self.worker_steps):
-            with torch.no_grad():
-                # `self.obs` keeps track of the last observation from each worker,
-                #  which is the input for the model to sample the next action
-                obs[:, t] = self.obs
-                # sample actions from $\pi_{\theta_{OLD}}$ for each worker;
-                #  this returns arrays of size `n_workers`
-                pi, v = self.model(obs_to_torch(self.obs))
-                values[:, t] = v.cpu().numpy()
-                a = pi.sample()
-                actions[:, t] = a.cpu().numpy()
-                log_pis[:, t] = pi.log_prob(a).cpu().numpy()
-
-            # run sampled actions on each worker
-            for w, worker in enumerate(self.workers):
-                worker.child.send(("step", actions[w, t]))
-
-            for w, worker in enumerate(self.workers):
-                # get results after executing the actions
-                self.obs[w], rewards[w, t], done[w, t], info = worker.child.recv()
-
-                # collect episode info, which is available if an episode finished;
-                #  this includes total reward and length of the episode -
-                #  look at `Game` to see how it works.
-                # We also add a game frame to it for monitoring.
-                if info:
-                    tracker.add('reward', info['reward'])
-                    tracker.add('length', info['length'])
-
-        # calculate advantages
-        advantages = self._calc_advantages(done, rewards, values)
-        samples = {
-            'obs': obs,
-            'actions': actions,
-            'values': values,
-            'log_pis': log_pis,
-            'advantages': advantages
-        }
-
-        # samples are currently in [workers, time] table,
-        #  we should flatten it
-        samples_flat = {}
-        for k, v in samples.items():
-            v = v.reshape(v.shape[0] * v.shape[1], *v.shape[2:])
-            if k == 'obs':
-                samples_flat[k] = obs_to_torch(v)
-            else:
-                samples_flat[k] = torch.tensor(v, device=device)
-
-        return samples_flat
-
-    def _calc_advantages(self, done: np.ndarray, rewards: np.ndarray, values: np.ndarray) -> np.ndarray:
-
-        # advantages table
-        advantages = np.zeros((self.n_workers, self.worker_steps), dtype=np.float32)
-        last_advantage = 0
-
-        # $V(s_{t+1})$
-        _, last_value = self.model(obs_to_torch(self.obs))
-        last_value = last_value.cpu().data.numpy()
-
-        for t in reversed(range(self.worker_steps)):
-            # mask if episode completed after step $t$
-            mask = 1.0 - done[:, t]
-            last_value = last_value * mask
-            last_advantage = last_advantage * mask
-            # $\delta_t$
-            delta = rewards[:, t] + self.gamma * last_value - values[:, t]
-
-            # $\hat{A_t} = \delta_t + \gamma \lambda \hat{A_{t+1}}$
-            last_advantage = delta + self.gamma * self.lamda * last_advantage
-
-            # note that we are collecting in reverse order.
-            # *My initial code was appending to a list and
-            #   I forgot to reverse it later.
-            # It took me around 4 to 5 hours to find the bug.
-            # The performance of the model was improving
-            #  slightly during initial runs,
-            #  probably because the samples are similar.*
-            advantages[:, t] = last_advantage
-
-            last_value = values[:, t]
-
-        return advantages
-
-    def train(self, samples: Dict[str, torch.Tensor], learning_rate: float, clip_range: float):
-        """
-        ### Train the model based on samples
-        """
-
-        # It learns faster with a higher number of epochs,
-        #  but becomes a little unstable; that is,
-        #  the average episode reward does not monotonically increase
-        #  over time.
-        # May be reducing the clipping range might solve it.
-        for _ in range(self.epochs):
-            # shuffle for each epoch
-            indexes = torch.randperm(self.batch_size)
-
-            # for each mini batch
-            for start in range(0, self.batch_size, self.mini_batch_size):
-                # get mini batch
-                end = start + self.mini_batch_size
-                mini_batch_indexes = indexes[start: end]
-                mini_batch = {}
-                for k, v in samples.items():
-                    mini_batch[k] = v[mini_batch_indexes]
-
-                # train
-                loss = self._calc_loss(clip_range=clip_range,
-                                       samples=mini_batch)
-
-                # compute gradients
-                for pg in self.optimizer.param_groups:
-                    pg['lr'] = learning_rate
-                self.optimizer.zero_grad()
-                loss.backward()
-                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
-                self.optimizer.step()
-
-    @staticmethod
-    def _normalize(adv: torch.Tensor):
-        """#### Normalize advantage function"""
-        return (adv - adv.mean()) / (adv.std() + 1e-8)
-
-    def _calc_loss(self, samples: Dict[str, torch.Tensor], clip_range: float) -> torch.Tensor:
-
-        sampled_return = samples['values'] + samples['advantages']
-        sampled_normalized_advantage = self._normalize(samples['advantages'])
-
-        pi, value = self.model(samples['obs'])
-
-        # #### Policy
-
-        log_pi = pi.log_prob(samples['actions'])
-        ratio = torch.exp(log_pi - samples['log_pis'])
-
-        clipped_ratio = ratio.clamp(min=1.0 - clip_range,
-                                    max=1.0 + clip_range)
-        policy_reward = torch.min(ratio * sampled_normalized_advantage,
-                                  clipped_ratio * sampled_normalized_advantage)
-        policy_reward = policy_reward.mean()
-
-        # #### Entropy Bonus
-
-        entropy_bonus = pi.entropy()
-        entropy_bonus = entropy_bonus.mean()
-
-        # #### Value
-
-        clipped_value = samples['values'] + (value - samples['values']).clamp(min=-clip_range,
-                                                                              max=clip_range)
-        vf_loss = torch.max((value - sampled_return) ** 2, (clipped_value - sampled_return) ** 2)
-        vf_loss = 0.5 * vf_loss.mean()
-
-        loss = -(policy_reward - 0.5 * vf_loss + 0.01 * entropy_bonus)
-
-        # for monitoring
-        approx_kl_divergence = .5 * ((samples['log_pis'] - log_pi) ** 2).mean()
-        clip_fraction = (abs((ratio - 1.0)) > clip_range).to(torch.float).mean()
-
-        tracker.add({'policy_reward': policy_reward,
-                     'vf_loss': vf_loss,
-                     'entropy_bonus': entropy_bonus,
-                     'kl_div': approx_kl_divergence,
-                     'clip_fraction': clip_fraction})
-
-        return loss
-
-    def run_training_loop(self):
-        """
-        ### Run training loop
-        """
-
-        # last 100 episode information
-        tracker.set_queue('reward', 100, True)
-        tracker.set_queue('length', 100, True)
-
-        for update in monit.loop(self.updates):
-            progress = update / self.updates
-
-            # decreasing `learning_rate` and `clip_range` $\epsilon$
-            learning_rate = 2.5e-4 * (1 - progress)
-            clip_range = 0.1 * (1 - progress)
-
-            # sample with current policy
-            samples = self.sample()
-
-            # train the model
-            self.train(samples, learning_rate, clip_range)
-
-            # write summary info to the writer, and log to the screen
-            tracker.save()
-            if (update + 1) % 1_000 == 0:
-                logger.log()
-
-    def destroy(self):
-        """
-        ### Destroy
-        Stop the workers
-        """
-        for worker in self.workers:
-            worker.child.send(("close", None))
-
-
-# ## Run it
-if __name__ == "__main__":
-    experiment.create()
-    m = Main()
-    experiment.start()
-    m.run_training_loop()
-    m.destroy()
\ No newline at end of file
diff --git a/pexps/scenarios/no_stop.py b/pexps/scenarios/no_stop.py
index e12b08e..6d4174a 100644
--- a/pexps/scenarios/no_stop.py
+++ b/pexps/scenarios/no_stop.py
@@ -1,6 +1,8 @@
+from numpy.lib.polynomial import roots
 from imported_utils import *
 from exp import *
 from env import *
+import math
 
 # Vehicle colors
 WHITE = (255, 255, 255)
@@ -21,33 +23,46 @@ class NoStopEnv(Env):
         ret = super().init_env()
         return ret
 
-    def step(self, action=[], rl_vehs=[], worker_id=0, warmup=False):
+    def step(self, action=[], rl_vehs=[], warmup=False):
         """
-        Build observation vectors and reward.
+        Build observations, rewards and also simulate the environment by one step.
         """
         c = self.c
         ts = self.ts
-
+        veh_edge = {}
+        # collect roads before doing an env step
+        for rl_id in rl_vehs:
+            veh_edge[rl_id] = ts.get_edge(rl_id)
         # set actions for rl vehicles
         if not warmup:
             for i in range(len(rl_vehs)):
-                ts.set_color(rl_vehs[i], RED)
                 act = action[i][0]
-                ts.accel(rl_vehs[i], act) 
+                ts.accel(rl_vehs[i], act)
 
         super().step()
-        # call this after step() function to get updated list
+        # call this after step() function to get updated vehicle list
         vehicle_list = ts.get_vehicle_list()
         obs, ids = self.get_state(vehicle_list, c, ts)
-        reward = self.get_reward(vehicle_list, c, ts, rl_vehs)
-        self.collect_stats(warmup, c, ts, vehicle_list, reward, worker_id)
-                
+        reward, _ = self.get_reward(vehicle_list, c, ts, rl_vehs, veh_edge)
+        self.collect_stats(warmup, c, ts, vehicle_list, reward)
+
         return Namespace(obs=obs, id=ids, reward=reward)
 
     def get_state(self, vehicle_list, c, ts):
+        """
+        State contains the following features.
+            - ego-vehicle distance to intersection
+            - ego vehicle speed
+            - leader relative distance
+            - leader speed
+            - folower relative distance
+            - follower speed
+            - traffic light phase
+            - traffic light time remaining in the current phase
+            - phase the ego-vehicle belongs to 
+        """
 
         obs = {}
-        speeds = []
         # "TL" is common to all vehicles 
         current_phase = ts.get_phase("TL")
         time_remaining = ts.remaining_phase_time("TL")
@@ -60,12 +75,8 @@ class NoStopEnv(Env):
                 tmp_obs = []
                 # rl vehicle speed and distance to and from the intersection
                 self_speed = ts.get_speed(veh)
-                self_distance, self_lane = ts.get_dist_intersection(veh)
+                self_distance, self_lane = ts.get_dist_intersection(veh, 250)
                 road_id = self_lane.split("_")[0]
-                if road_id not in incoming_roads:
-                    tmp_obs.append(0) # indicate outgoing lane 
-                else: 
-                    tmp_obs.append(1) # indicate incoming lane 
                 tmp_obs.append(self_distance/250) # rl position
                 tmp_obs.append(self_speed/c.target_vel)
 
@@ -73,13 +84,17 @@ class NoStopEnv(Env):
                 # TODO limit the visible distance to d = 100m
                 leader_info = ts.get_leader(veh)
                 if leader_info == None:
-                    # fill with padding values
-                    leader_speed = c.target_vel # padding with maximum velocity
-                    leader_dist = 250 # padding with max
+                    if road_id not in incoming_roads:
+                        leader_speed = c.target_vel # padding with maximum velocity
+                        leader_dist = 250 # padding with max
+                    else:
+                        # at this point we know that 'veh' is a leading vehicle in an incoming approach
+                        leader_speed = c.target_vel # padding with maximum velocity
+                        leader_dist = 250 # padding with max
                 else:
                     leader_id, leader_dist = leader_info
                     leader_speed = ts.get_speed(leader_id)
-                tmp_obs.append(leader_speed/ c.target_vel)
+                tmp_obs.append(leader_speed/c.target_vel)
                 tmp_obs.append(leader_dist/250)
                 
                 # follower speed and relative distance
@@ -98,90 +113,199 @@ class NoStopEnv(Env):
                 if road_id not in incoming_roads:
                     tmp_obs.append(2) #padding
                     tmp_obs.append(2) #padding
+                    tmp_obs.append(2) #padding
                 else:
                     if current_phase == 0:
+                        tmp_obs.append(1) # red light
                         tmp_obs.append(1) # next phase
                         tmp_obs.append((time_remaining + 4)/34)
                     elif current_phase == 2:
+                        tmp_obs.append(1) # red light
                         tmp_obs.append(0) # next phase
                         tmp_obs.append((time_remaining + 4)/34)
                     elif current_phase == 1:
+                        tmp_obs.append(0) # yellow light
                         tmp_obs.append(1) # next phase
                         tmp_obs.append(time_remaining/34)
                     elif current_phase == 3:
+                        tmp_obs.append(0) # yellow light
                         tmp_obs.append(0) # next phase
                         tmp_obs.append(time_remaining/34)
-                
-                phase_id_index = 0
+
                 if road_id == "E2TL":
-                    phase_id_index = 1
+                    # phase_id_index = 1
                     tmp_obs.append(0) 
                     tmp_obs.append(0) 
                 elif road_id == "N2TL":
-                    phase_id_index = 2
+                    # phase_id_index = 2
                     tmp_obs.append(0) 
                     tmp_obs.append(1) 
                 elif road_id == "W2TL":
-                    phase_id_index = 1
+                    # phase_id_index = 1
                     tmp_obs.append(0) 
                     tmp_obs.append(0) 
                 elif road_id == "S2TL":
-                    phase_id_index = 2
+                    # phase_id_index = 2
                     tmp_obs.append(0) 
                     tmp_obs.append(1) 
                 elif road_id.startswith( ':TL' ):
                     # middle of intersection 
-                    phase_id_index = 3
+                    # phase_id_index = 3
                     tmp_obs.append(1) 
                     tmp_obs.append(0) 
                 else:
                     # in an outgoing lane
-                    phase_id_index = 4
+                    # phase_id_index = 4
                     tmp_obs.append(1) 
                     tmp_obs.append(1) 
-
                 obs[veh] = np.array(tmp_obs)
 
-            else:
-                self_speed = ts.get_speed(veh)
-            speeds.append(self_speed)
-
         sort_id = lambda d: [v for k, v in sorted(d.items())]
         ids = sorted(obs)
         obs = arrayf(sort_id(obs)).reshape(-1, c._n_obs)
-
         return obs, ids
 
-    def get_reward(self, vehicle_list, c, ts, rl_vehs):
-        # build the reward 
-        all_speeds = []
-        rl_speeds = [] 
-        all_accels = []
-        rl_accels = []
+    def fuel_model(self, v_speed, v_accel):
+        """VT-CPFM Fuel Model"""
+        R_f = 1.23*0.6*0.98*3.28*(v_speed**2) + 9.8066*3152*(1.75/1000)*0.033*v_speed + 9.8066*3152*(1.75/1000)*0.033 + 9.8066*3152*0
+        power = ((R_f + 1.04*3152*v_accel)/(3600*0.92)) * v_speed
+        fuel = 0
+        if power >= 0:
+            fuel = 0.00078 + 0.000019556*power + 0.000001*(power**2)
+        else:
+            fuel = 0.00078
+        return fuel
+
+
+    def get_reward(self, vehicle_list, c, ts, old_vehicle_list, veh_edge):
+        """Compute the reward of the previous action."""
+
+        max_speed = c.target_vel
+        
+        horizontal_speeds = []
+        vertical_speeds = []
+        horizontal_fuels = []
+        vertical_fuels = []
 
         for veh in vehicle_list:
-            v_accel = ts.get_acceleration(veh)
             v_speed = ts.get_speed(veh)
-            if veh.startswith('rl'):
-                rl_speeds.append(v_speed)
-                rl_accels.append(v_accel)
-            all_speeds.append(v_speed)
-            all_accels.append(v_accel)
+            v_accel = ts.get_acceleration(veh)
+            v_edge = ts.get_edge(veh)
+
+            if v_edge.startswith("E2TL") or v_edge.startswith("TL2W"):
+                horizontal_speeds.append(v_speed)
+            else:
+                vertical_speeds.append(v_speed)
 
-        reward = 0
-        if len(all_speeds) != 0:
-            reward =  np.mean(np.array(all_speeds))/c.target_vel
+            # virginia fuel model
+            fuel = self.fuel_model(v_speed, v_accel)
 
-        # normalize the reward with available agents 
-        reward = reward / len(rl_vehs)
-        return reward
+            if v_edge.startswith("E2TL") or v_edge.startswith("TL2W"):
+                horizontal_fuels.append(fuel)
+            else:
+                vertical_fuels.append(fuel)
+
+        # stop reward
+        horizontal_stops = 0
+        vertical_stops = 0
+        h_vehs = 0
+        v_vehs = 0
+        for veh in vehicle_list:
+            tmp_speed = ts.get_speed(veh)
+            tmp_edge = ts.get_edge(veh)
+            if tmp_edge.startswith("E2TL") or tmp_edge.startswith("TL2W"):
+                h_vehs += 1
+            else:
+                v_vehs += 1
+            if tmp_speed < 0.3:
+                if tmp_edge.startswith("E2TL") or tmp_edge.startswith("TL2W"):
+                    horizontal_stops += 1
+                else:
+                    vertical_stops += 1 
 
-    def collect_stats(self, warmup, c, ts, vehicle_list, reward, worker_id):
+        slow_v_veh = False
+        slow_h_veh = False 
+        for veh in vehicle_list:
+            v_edge = ts.get_edge(veh)
+            if v_edge.startswith("E2TL") :
+                v_pos = ts.get_position(veh)
+                v_speed = ts.get_speed(veh)
+                if v_pos < 250 - 120:
+                    if v_speed < 5 and not slow_h_veh:
+                        slow_h_veh = True
+            elif v_edge.startswith("S2TL") :
+                v_pos = ts.get_position(veh)
+                v_speed = ts.get_speed(veh)
+                if v_pos < 250 - 120:
+                    if v_speed < 5 and not slow_v_veh:
+                        slow_v_veh = True
+
+        lead_h_speed = 0
+        lead_v_speed = 0
+        for veh in vehicle_list:
+            leader_info = ts.get_leader(veh)
+            v_speed = ts.get_speed(veh)
+            v_edge = ts.get_edge(veh)
+            if leader_info == None:
+                if v_edge.startswith("S2TL") and lead_v_speed == 0:
+                    lead_v_speed = v_speed
+                elif v_edge.startswith("E2TL") and lead_h_speed == 0:
+                    lead_h_speed = v_speed
+
+        stop_h_reward = 0
+        stop_v_reward = 0
+        if h_vehs != 0:
+            stop_h_reward = -1*(horizontal_stops)/h_vehs
+        if v_vehs != 0:
+            stop_v_reward = -1*(vertical_stops)/v_vehs
+            
+        avg_h_speed_reward = np.mean(np.array(horizontal_speeds)/max_speed)
+        avg_v_speed_reward = np.mean(np.array(vertical_speeds)/max_speed)
+
+        avg_h_fuel_reward = (np.mean(np.array(horizontal_fuels)))
+        avg_v_fuel_reward = (np.mean(np.array(vertical_fuels)))
+
+        if slow_h_veh:
+            tot_h_reward = -100
+        elif avg_h_fuel_reward <= 0.00078 and horizontal_stops == 0:
+            tot_h_reward = 5*(math.exp(avg_h_speed_reward) - 1)
+        elif avg_h_fuel_reward <= 0.00078 and horizontal_stops > 0:
+            tot_h_reward = 5*(math.exp(avg_h_speed_reward) - 1) + 10*stop_h_reward
+        else:
+            #tot_h_reward = -3*(math.exp(1000*avg_h_fuel_reward)-1) + 4*(math.exp(avg_h_speed_reward) - 1) + 10*stop_h_reward
+            tot_h_reward = -3.5*(math.exp(1000*avg_h_fuel_reward)-1.1) + 5*(math.exp(avg_h_speed_reward) - 1) + 10*stop_h_reward
+
+        if slow_v_veh:
+            tot_v_reward = -100
+        elif avg_v_fuel_reward <= 0.00078 and vertical_stops == 0:
+            tot_v_reward = 5*(math.exp(avg_v_speed_reward) - 1)
+        elif avg_v_fuel_reward <= 0.00078 and vertical_stops > 0:
+            tot_v_reward = 5*(math.exp(avg_v_speed_reward) - 1) + 10*stop_v_reward
+        else:
+            #tot_v_reward = -3*(math.exp(1000*avg_v_fuel_reward)-1) + 4*(math.exp(avg_v_speed_reward) - 1) + 10*stop_v_reward
+            tot_v_reward = -3.5*(math.exp(1000*avg_v_fuel_reward)-1.1) + 5*(math.exp(avg_v_speed_reward) - 1) + 10*stop_v_reward
+
+        rewards = {}
+        
+        for rl_id in old_vehicle_list:
+            tmp_edge = veh_edge[rl_id]
+            if tmp_edge.startswith("E2TL") or tmp_edge.startswith("TL2W"):
+                rewards[rl_id] = tot_h_reward/h_vehs
+            else:
+                rewards[rl_id] = tot_v_reward/v_vehs
+
+        sort_id = lambda d: [v for k, v in sorted(d.items())]
+        ids = sorted(rewards)
+        rewards = arrayf(sort_id(rewards)).reshape(-1, 1)
+        return rewards, ids
+
+    def collect_stats(self, warmup, c, ts, vehicle_list, reward):
         # collect stats
         all_speeds = []
         rl_speeds = [] 
         all_accels = []
         rl_accels = []
+        all_fuel = []
 
         for veh in vehicle_list:
             v_accel = ts.get_acceleration(veh)
@@ -191,15 +315,22 @@ class NoStopEnv(Env):
                 rl_accels.append(v_accel)
             all_speeds.append(v_speed)
             all_accels.append(v_accel)
+            all_fuel.append(self.fuel_model(v_speed, v_accel))
+
 
         if not warmup:
-            c.avg_reward += reward
+            c.avg_reward += reward[0][0]
             c.running_steps += 1
 
             if len(all_speeds) == 0:
                 avg_speed = 0
             else:
                 avg_speed = np.mean(np.array(all_speeds))
+            
+            if len(all_fuel) == 0:
+                sum_fuel = 0
+            else:
+                sum_fuel = np.sum(np.array(all_fuel))
 
             if len(rl_speeds) == 0:
                 avg_rl_speed = 0
@@ -208,6 +339,8 @@ class NoStopEnv(Env):
 
             c.velocity_fleet += avg_speed
             c.rl_speed += avg_rl_speed
+            c.fuel_fleet += sum_fuel
+        
 
 class NoStopNetwork(Main):
 
@@ -223,33 +356,6 @@ class NoStopNetwork(Main):
     @property
     def action_space(c):
         if c.act_type == 'accel':
-            return Box(low=-3, high=3, shape=(1,), dtype=np.float32)
+            return Box(low=c.max_decel, high=c.max_accel, shape=(1,), dtype=np.float32)
         else:
-            return Discrete(c.n_actions)
-
-    def on_rollout_end(c, rollout, stats, ii=None, n_ii=None):
-        log = c.get_log_ii(ii, n_ii)
-        step_obs_ = rollout.obs
-        step_obs = step_obs_[:-1]
-
-        rollout.raw_reward = rollout.reward
-        #rollout.reward = [c._norm.norm_reward(r) for r in rollout.raw_reward]
-
-        ret, _ = calc_adv(rollout.reward, c.gamma)
-
-        n_veh = np.array([len(o) for o in step_obs])
-        step_ret = [[r] * nv for r, nv in zip(ret, n_veh)]
-        rollout.update(obs=step_obs, ret=step_ret)
-
-        step_id_ = rollout.pop('id')
-        id = np.concatenate(step_id_[:-1])
-        id_unique = np.unique(id)
-
-        reward = np.array(rollout.pop('reward'))
-        raw_reward = np.array(rollout.pop('raw_reward'))
-
-        log(**stats)
-        log(raw_reward_mean=raw_reward.mean(), raw_reward_sum=raw_reward.sum())
-        log(reward_mean=reward.mean(), reward_sum=reward.sum())
-        log(n_veh_step_mean=n_veh.mean(), n_veh_step_sum=n_veh.sum(), n_veh_unique=len(id_unique))
-        return rollout
\ No newline at end of file
+            return Discrete(c.n_actions)
\ No newline at end of file
diff --git a/pexps/sumo_utils/sumo.py b/pexps/sumo_utils/sumo.py
index 3e7f817..c2fc36a 100644
--- a/pexps/sumo_utils/sumo.py
+++ b/pexps/sumo_utils/sumo.py
@@ -215,104 +215,52 @@ class TrafficState:
             warnings.warn('Caught the following exception while adding vehicle %s, removing and readding vehicle:\n%s' % (veh_id, e))
             self.remove(veh_id)
             self.add(veh_id, route, type, lane_index, pos, speed, patience=patience - 1)
-
-    def set_color(self, veh, color):
-        self.tc.vehicle.setColor(veh, color + (255,))
-
-    def slow_down(self, veh, new_target_speed, duration):
-        self.tc.vehicle.slowDown(veh, new_target_speed, duration)
-
-    def accel(self, veh, acc, n_acc_steps=1):
-        """
-        Let the initial speed be v0, the sim_step be dt, and the acceleration be a. 
-        This function increases v0 over n=n_acc_steps steps by a*dt/n per step. 
-        At each of the sim steps, the speed increases by a*dt/n at the BEGINNING of the step. 
-        After one step, the vehicle's speed is v1=v0+a*dt/n and the distance traveled is v1*dt. 
-        If n>1, then after two steps, the vehicle's speed is v2=v1+a*dt/n and the distance traveled is v2*dt. Etc etc. 
-        If accel is called again before n steps has elapsed, the new acceleration action
-        overrides the continuation of any previous acceleration. 
-        The per step acceleration a/n is clipped by SUMO's IDM behavior to be in the range 
-        of -max_decel <= a/n <= max_accel, where max_accel and max_decel are the IDM parameters given to SUMO.
-        """
-        speed = self.get_speed(veh)
-        self.tc.vehicle.slowDown(veh, max(0, speed + acc * self.c.sim_step), 1e-3)
-
-    def set_max_speed(self, veh, speed):
-        self.tc.vehicle.setMaxSpeed(veh, max(speed, 1e-3))
     
     def get_max_speed(self, veh):
         self.tc.vehicle.getMaxSpeed(veh)
 
-    def lane_change(self, veh, lane_index, direction):
-        assert direction in [-1, 0, 1]
-        self.tc.vehicle.changeLane(veh, lane_index + int(direction), 100000.0)
-
-    def lane_change_to(self, veh, lane_index):
-        self.tc.vehicle.changeLane(veh, lane_index, 100000.0)
-
-    def set_program(self, tl, program):
-        self.tc.trafficlight.setProgram(tl, program)
-
     def get_program(self, tl):
         return self.tc.trafficlight.getProgram(tl)
 
-    def set_phase(self, tl, phase_index):
-        return self.tc.trafficlight.setPhase(tl, phase_index)
-
     def get_phase(self, tl):
         return self.tc.trafficlight.getPhase(tl)
-    
+
     def get_vehicle_list(self):
-        return self.tc.vehicle.getIDList()
+        return self.vehicles.keys()
 
-    def get_speed(self, veh):
-        return self.tc.vehicle.getSpeed(veh)
+    def get_speed(self, veh): 
+        return self.vehicles[veh].speed
     
-    def get_position(self, veh):
-        return self.tc.vehicle.getLanePosition(veh)
+    def get_position(self, veh): 
+        return self.vehicles[veh].laneposition
 
-    def get_road_id(self, veh):
-        return self.tc.vehicle.getRoadID(veh)
+    def get_edge(self, veh): 
+        return self.vehicles[veh].road_id
 
-    def get_dist_intersection(self, veh):
-        lane_id = self.tc.vehicle.getLaneID(veh)
-        lane_pos = self.tc.vehicle.getLanePosition(veh)
+    def get_dist_intersection(self, veh, lane_length): 
+        lane_id = self.vehicles[veh].lane_id
+        lane_pos = self.vehicles[veh].laneposition
         # invert lane position value,
         # so if the car is close to the traffic light -> lane_pos = 0 -> 750 = max len of a road
         # https://sumo.dlr.de/pydoc/traci._vehicle.html#VehicleDomain-getLanePosition
-        return self.get_lane_length(lane_id) - lane_pos, lane_id
+        return lane_length - lane_pos, lane_id
+
+    def get_acceleration(self, veh): 
+        return self.vehicles[veh].accel
     
-    def get_lane_length(self, lane_id):
-        if lane_id not in self.lane_lengths.keys():
-            self.lane_lengths[lane_id] = self.tc.lane.getLength(lane_id) 
-        return self.lane_lengths[lane_id]
+    def get_lane_id(self, veh): 
+        return self.vehicles[veh].lane_id
 
-    def remaining_phase_time(self, tl):
+    def remaining_phase_time(self, tl): 
         return self.tc.trafficlight.getNextSwitch(tl) - self.tc.simulation.getTime()
     
-    def get_co2_emission(self, veh):
-        return self.tc.vehicle.getCO2Emission(veh)
-
-    def get_chached_speed(self, veh):
-        return self.vehicles[veh].speed
-
-    def get_dist_intersection(self, veh):
-        lane_id = self.vehicles[veh].lane_id
-        lane_pos = self.get_position(veh)
-        # invert lane position value,
-        # so if the car is close to the traffic light -> lane_pos = 0 -> 750 = max len of a road
-        # https://sumo.dlr.de/pydoc/traci._vehicle.html#VehicleDomain-getLanePosition
-        return self.get_lane_length(lane_id) - lane_pos, lane_id
+    def get_co2_emission(self, veh): 
+        return self.vehicles[veh].co2emission
 
-    def get_chached_dist_intersection(self, veh):
-        lane_id = self.vehicles[veh].lane_id
-        lane_pos = self.vehicles[veh].laneposition
-        # invert lane position value,
-        # so if the car is close to the traffic light -> lane_pos = 0 -> 750 = max len of a road
-        # https://sumo.dlr.de/pydoc/traci._vehicle.html#VehicleDomain-getLanePosition
-        return self.get_lane_length(lane_id) - lane_pos, lane_id
+    def get_analytical_co2_emission(self, veh):
+        return self.vehicles[veh].analytical_co2_emit
 
-    def get_chached_leader(self, veh):
+    def get_leader(self, veh): 
         lane_id = self.vehicles[veh].lane_id
         lane_pos = self.vehicles[veh].laneposition
 
@@ -333,28 +281,8 @@ class TrafficState:
             return None
         return keys[1], sorted_candidates[keys[1]]
 
-    def get_leader(self, veh):
-        lane_id = self.get_lane_id(veh)
-        lane_pos = self.get_position(veh)
-
-        candidates = {}
-
-        for veh_id in self.get_vehicle_list():
-            v_lane_id = self.get_lane_id(veh_id)
-            if v_lane_id == lane_id:
-                v_lane_pos = self.get_position(veh_id)
-                if v_lane_pos >= lane_pos:
-                    candidates[veh_id] = v_lane_pos - lane_pos
-        # if empty, return None            
-        if not bool(candidates):
-            return None 
-        sorted_candidates = {k: v for k, v in sorted(candidates.items(), key=lambda item: item[1])}
-        keys = list(sorted_candidates)
-        if len(keys) == 1:
-            return None
-        return keys[1], sorted_candidates[keys[1]]
 
-    def get_chached_follower(self, veh):
+    def get_follower(self, veh): 
         lane_id = self.vehicles[veh].lane_id
         lane_pos = self.vehicles[veh].laneposition
 
@@ -374,51 +302,43 @@ class TrafficState:
         if len(keys) == 1:
             return None
         return keys[1], sorted_candidates[keys[1]]
+        
+    def set_speed(self, veh, speed):
+        self.tc.vehicle.setSpeed(veh, speed)
 
-    def get_lane_id(self, veh):
-        return self.tc.vehicle.getLaneID(veh)
-
-    def get_follower(self, veh):
-        lane_id = self.get_lane_id(veh)
-        lane_pos = self.get_position(veh)
+    def set_accel(self, veh, accel):
+        self.tc.vehicle.setAccel(veh, accel)
 
-        candidates = {}
+    def set_phase(self, tl, phase_index):
+        return self.tc.trafficlight.setPhase(tl, phase_index)
 
-        for veh_id in self.get_vehicle_list():
-            v_lane_id = self.get_lane_id(veh_id)
-            if v_lane_id == lane_id:
-                v_lane_pos = self.get_position(veh_id)
-                if v_lane_pos <= lane_pos:
-                    candidates[veh_id] = lane_pos - v_lane_pos
-        # if empty, return None            
-        if not bool(candidates):
-            return None 
-        sorted_candidates = {k: v for k, v in sorted(candidates.items(), key=lambda item: item[1])}
-        keys = list(sorted_candidates)
-        if len(keys) == 1:
-            return None
-        return keys[1], sorted_candidates[keys[1]]
+    def set_program(self, tl, program):
+        self.tc.trafficlight.setProgram(tl, program)
 
-    def get_cached_co2_emission(self, veh):
-        return self.vehicles[veh].co2emission
-    
-    def get_cached_total_co2_emission(self, veh):
-        return self.vehicles[veh].total_co2_emission/10e5
-        
-    def set_speed(self, veh, speed):
-        self.tc.vehicle.setSpeed(veh, speed)
+    def set_color(self, veh, color):
+        self.tc.vehicle.setColor(veh, color + (255,))
 
-    def get_acceleration(self, veh):
-        return self.tc.vehicle.getAcceleration(veh)
+    def accel(self, veh, acc, n_acc_steps=1):
+        """
+        Let the initial speed be v0, the sim_step be dt, and the acceleration be a. 
+        This function increases v0 over n=n_acc_steps steps by a*dt/n per step. 
+        At each of the sim steps, the speed increases by a*dt/n at the BEGINNING of the step. 
+        After one step, the vehicle's speed is v1=v0+a*dt/n and the distance traveled is v1*dt. 
+        If n>1, then after two steps, the vehicle's speed is v2=v1+a*dt/n and the distance traveled is v2*dt. Etc etc. 
+        If accel is called again before n steps has elapsed, the new acceleration action
+        overrides the continuation of any previous acceleration. 
+        The per step acceleration a/n is clipped by SUMO's IDM behavior to be in the range 
+        of -max_decel <= a/n <= max_accel, where max_accel and max_decel are the IDM parameters given to SUMO.
+        """
+        speed = self.get_speed(veh)
+        self.tc.vehicle.slowDown(veh, max(0, speed + acc * self.c.sim_step), 1e-3)
 
-    def get_analytical_co2_emission(self, veh):
-        return self.vehicles[veh].analytical_co2_emit
+    def set_max_speed(self, veh, speed):
+        self.tc.vehicle.setMaxSpeed(veh, max(speed, 1e-3))
 
-    def set_accel(self, veh, accel):
-        self.tc.vehicle.setAccel(veh, accel)
-    
-    def get_accel(self, veh):
-        self.tc.vehicle.getAcceleration(veh)
+    def lane_change(self, veh, lane_index, direction):
+        assert direction in [-1, 0, 1]
+        self.tc.vehicle.changeLane(veh, lane_index + int(direction), 100000.0)
 
-    def get_vehicle_list(self):
-        return self.tc.vehicle.getIDList()
\ No newline at end of file
+    def lane_change_to(self, veh, lane_index):
+        self.tc.vehicle.changeLane(veh, lane_index, 100000.0)
\ No newline at end of file
diff --git a/pexps/ut.py b/pexps/ut.py
index 1fe9ab8..2e4b050 100644
--- a/pexps/ut.py
+++ b/pexps/ut.py
@@ -117,6 +117,7 @@ class NamedArrays(dict):
         return type(self)((k, v) for k, v in self.items() if k in args)
 
     def iter_minibatch(self, n_minibatches=None, concat=False, device='cpu'):
+        
         if n_minibatches in [1, None]:
             yield slice(None), self.to_array(inplace=False, concat=concat).to_torch(device=device)
         else:
@@ -152,7 +153,10 @@ class NamedArrays(dict):
             return cls()
         def concat(xs):
             """
-            Common error with np.concatenate: conside arrays a and b, both of which are lists of arrays. If a contains irregularly shaped arrays and b contains arrays with the same shape, the numpy will treat b as a 2D array, and the concatenation will fail. Solution: use flatten instead of np.concatenate for lists of arrays
+            Common error with np.concatenate: conside arrays a and b, both of which are lists of arrays. 
+            If a contains irregularly shaped arrays and b contains arrays with the same shape, the numpy 
+            will treat b as a 2D array, and the concatenation will fail. 
+            Solution: use flatten instead of np.concatenate for lists of arrays
             """
             try: return np.concatenate(xs)
             except: return flatten(xs)
@@ -281,7 +285,7 @@ class FFN(nn.Module):
         self.p_head = build_fc(s_sizes[-1], *layers.p, c.model_output_size)
         self.sequential_init(self.p_head, 'policy')
         self.v_head = None
-        if c.use_critic:
+        if True:#c.use_critic:
             self.v_head = build_fc(s_sizes[-1], *layers.v, 1)
             self.sequential_init(self.v_head, 'value')
 
@@ -326,10 +330,11 @@ def calc_adv(reward, gamma, value_=None, lam=None):
     if value_ is None:
         return discount(reward, gamma), None # TD(1)
     if isinstance(reward, list):
-        reward, value_ = map(np.array, (reward, value_))
+        reward = np.array([np.array(x) for x in reward])
+        value_ = np.array([np.array(x) for x in value_])
     assert value_.ndim == reward.ndim == 1, f'Value and reward be one dimensional, but got {value_.shape} and {reward.shape} respectively'
     assert value_.shape[0] - reward.shape[0] in [0, 1], f'Value\'s shape can be at most 1 bigger than reward\'s shape, but got {value_.shape} and {reward.shape} respectively'
-
+    
     if value_.shape[0] == reward.shape[0]:
         delta = reward - value_
         delta[:-1] += gamma * value_[1:]
@@ -339,6 +344,15 @@ def calc_adv(reward, gamma, value_=None, lam=None):
     ret = value_[:len(adv)] + adv
     return ret, adv
 
+def discount_rewards(rewards, gamma=0.99):
+    """
+    Return discounted rewards based on the given rewards and gamma param.
+    """
+    new_rewards = [float(rewards[-1])]
+    for i in reversed(range(len(rewards)-1)):
+        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])
+    return np.array(new_rewards[::-1])
+
 def calc_adv_multi_agent(id_, reward, gamma, value_=None, lam=None):
     """
     Calculate advantage with TD-lambda for multiple agents
@@ -357,6 +371,7 @@ def calc_adv_multi_agent(id_, reward, gamma, value_=None, lam=None):
         ret[idxs], adv[idxs] = calc_adv(reward=reward[idxs], gamma=gamma, value_=value_i_, lam=lam)
     return ret, adv
 
+
 class Algorithm:
     """
     RL algorithm interface
@@ -391,78 +406,10 @@ class Algorithm:
             torch.nn.utils.clip_grad_norm_(c._model.parameters(), c.normclip)
         c._opt.step()
 
-class ValueFitting(Algorithm):
-    def __init__(self, c):
-        super().__init__(c.setdefaults(vclip=None))
-
-    def optimize(self, rollouts):
-        c = self.c
-        buffer = c._buffer[:c._buffer_size]
-        for i_gd in range(c.n_gds):
-            batch_stats = []
-            for idxs, mb in buffer.iter_minibatch(max(1, c._buffer_size // c.size_mb), concat=c.batch_concat, device=c.device):
-                pred = c._model(mb.obs, value=True)
-                value_mask = mb.obs.get('value_mask') if isinstance(mb.obs, dict) else None
-                loss = self.value_loss(pred.value, mb.ret, v_start=mb.value, mask=value_mask)
-                self.step_loss(loss)
-                batch_stats.append(from_torch(dict(loss=loss)))
-            c.log_stats(pd.DataFrame(batch_stats).mean(axis=0), ii=i_gd, n_ii=c.n_gds)
-
-class Imitation(Algorithm):
-    def __init__(self, c):
-        super().__init__(c.setdefaults(n_gds=10, lr=1e-3, vcoef=1, vclip=None))
-
-    def optimize(self, rollouts):
-        c = self.c
-        buffer = c._buffer[:c._buffer_size]
-
-        for i_gd in range(c.n_gds):
-            batch_stats = NamedArrays()
-            for _, mb in buffer.iter_minibatch(max(1, c._buffer_size // c.size_mb), concat=c.batch_concat, device=c.device):
-                pred = c._model(mb.obs, policy=True, value=True)
-                curr_dist = c.dist_class(pred.policy)
-                label = mb.label
-                n = len(label)
-                n_pos = label.sum()
-                n_neg = n - n_pos
-                if isinstance(c.action_space, Box):
-                    z = torch.normal(0, 1, (n, 10), device=c.device) # pathwise estimator
-                    samples = curr_dist.mean + curr_dist.std * z
-                    samples = (samples.clamp(c.low, 1) - c.low) / (1 - c.low)
-                    mse = ((label.view(-1, 1) - samples) ** 2).mean(dim=1)
-                    entropy = curr_dist.entropy().mean()
-                    ent_loss = -c.entcoef * entropy
-                    if c.get('imitation_balance'):
-                        pos_loss = label.dot(mse) / n_pos if n_pos > 0 else 0
-                        neg_loss = (1 - label).dot(mse) / n_neg if n_neg > 0 else 0
-                        policy_loss = pos_loss + neg_loss + ent_loss
-                        batch_stats.append(policy_loss=policy_loss,n_pos=n_pos, pos_loss=pos_loss, n_neg=n_neg, neg_loss=neg_loss, entropy=entropy, ent_loss=ent_loss, std=curr_dist.std.mean())
-                    else:
-                        mse_loss = mse.mean()
-                        policy_loss = mse_loss + ent_loss
-                        batch_stats.append(policy_loss=policy_loss, mse_loss=mse_loss, entropy=entropy, ent_loss=ent_loss, std=curr_dist.std.mean())
-                else:
-                    logp = curr_dist.logp(mb.label.long())
-                    if c.get('imitation_balance'):
-                        pos_loss = -label.dot(logp) / n_pos
-                        neg_loss = -(1 - label).dot(logp) / n_neg
-                        policy_loss = pos_loss + neg_loss
-                        batch_stats.append(policy_loss=policy_loss, pos_loss=pos_loss, neg_loss=neg_loss)
-                    else:
-                        policy_loss = logp.mean()
-                        batch_stats.append(policy_loss=policy_loss)
-                if c.use_critic:
-                    value_loss = self.value_loss(pred.value, mb.ret, v_start=mb.value, mask=mb.obs.get('value_mask'))
-                    batch_stats.append(value_loss=value_loss)
-                    loss = policy_loss + value_loss * c.vcoef
-                else:
-                    loss = policy_loss
-                self.step_loss(loss)
-            c.log_stats(dict(ii=i_gd, n_ii=c.n_gds, **pd.DataFrame(from_torch(batch_stats)).mean(axis=0)))
 
 class PPO(Algorithm):
     def __init__(self, c):
-        super().__init__(c.setdefaults(use_critic=True, n_gds=30, pclip=0.3, vcoef=1, vclip=1, klcoef=0.2, kltarg=0.02, entcoef=0))
+        super().__init__(c.setdefaults(use_critic=True, n_gds=10, pclip=0.3, vcoef=1, vclip=10.0, klcoef=0.2, kltarg=0.02, entcoef=0))
 
     def on_step_start(self):
         stats = dict(klcoef=self.c.klcoef)
@@ -480,6 +427,7 @@ class PPO(Algorithm):
         batch = rollouts.filter('obs', 'policy', 'action', 'pg_obj', 'ret', *lif(c.use_critic, 'value', 'adv'))
         value_warmup = c._i < c.get('n_value_warmup', 0)
 
+        stop_update = False
         for i_gd in range(c.n_gds):
             batch_stats = []
             for idxs, mb in batch.iter_minibatch(c.get('n_minibatches'), concat=c.batch_concat, device=c.device):
@@ -520,7 +468,15 @@ class PPO(Algorithm):
                     stats['value_loss'] = value_loss
                 self.step_loss(loss)
                 batch_stats.append(from_torch(stats))
+
+                if kl >= c.kltarg:
+                    print("Trust region may have been reached!")
+                    stop_update = True
+                    break
+
             c.log_stats(pd.DataFrame(batch_stats).mean(axis=0), ii=i_gd, n_ii=c.n_gds)
+            if stop_update:
+                break
 
         if c.klcoef:
             kl = from_torch(kl)
@@ -529,10 +485,6 @@ class PPO(Algorithm):
             elif kl < 0.5 * c.kltarg:
                 c.klcoef *= 0.5
 
-class PG(PPO):
-    def __init__(self, c):
-        super().__init__(c.var(n_gds=c.n_gds, use_critic=False))
-
 class TRPO(Algorithm):
     def __init__(self, c):
         if c.get('max_kl'):
diff --git a/scripts/setup_aimsun.sh b/scripts/setup_aimsun.sh
old mode 100755
new mode 100644
diff --git a/scripts/setup_sumo_osx.sh b/scripts/setup_sumo_osx.sh
old mode 100755
new mode 100644
diff --git a/scripts/setup_sumo_ubuntu1404.sh b/scripts/setup_sumo_ubuntu1404.sh
old mode 100755
new mode 100644
diff --git a/scripts/setup_sumo_ubuntu1604.sh b/scripts/setup_sumo_ubuntu1604.sh
old mode 100755
new mode 100644
diff --git a/scripts/setup_sumo_ubuntu1804.sh b/scripts/setup_sumo_ubuntu1804.sh
old mode 100755
new mode 100644
diff --git a/submit.sh b/submit.sh
index d93356d..0bda791 100644
--- a/submit.sh
+++ b/submit.sh
@@ -1,11 +1,13 @@
 #!/bin/sh
 
-#SBATCH -o output-%j.log
+#SBATCH -o R1-newsumo-newR-%a.log
+#SBATCH --job-name=R1-newsumo-newR
+#SBATCH -a 1-1
 #SBATCH -N 1                     # number of nodes
-#SBATCH -n 1                     # number of tasks
+#SBATCH -n 16                    # number of tasks
 #SBATCH -c 1                     # number of cpu per task
 #SBATCH --time=72:00:00          # total run time limit (HH:MM:SS)
 
 source ~/.bash_profile
 export OMP_NUM_THREADS=1
-python -u $F/pexps/platoon.py .
+python -u $F/pexps/main.py --res R1-newsumo-newR
\ No newline at end of file
diff --git a/sumo/routes.rou.xml b/sumo/routes.rou.xml
index c60b78b..53a55e1 100644
--- a/sumo/routes.rou.xml
+++ b/sumo/routes.rou.xml
@@ -10,5 +10,6 @@
             <route id="E_W" edges="E2TL TL2W"/>
             <route id="S_N" edges="S2TL TL2N"/>
 
-            <flow id="rl_S_N_" vehsPerHour="800" route="S_N" departSpeed="12"/>
+            <flow id="rl_S_N_" vehsPerHour="800" route="S_N" departSpeed="10"/>
+            <flow id="rl_E_W_" vehsPerHour="800" route="E_W" departSpeed="10"/>
 </routes>